(*
   Copyright 2015:
     Leonid Rozenberg <leonidr@gmail.com>

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
*)

open Test_utils

module U = Univariate
module M = Multivariate
module T = Tikhonov

let looe_manually lambda pred resp =
  let predi = Array.to_list pred |> List.mapi (fun i p -> (i, p)) in
  let respi = Array.to_list resp |> List.mapi (fun i r -> (i, r)) in
  let without i =
    List.filter (fun (j, _) -> j <> i) predi |> Array.of_list |> Array.map snd,
    List.filter (fun (j, _) -> j <> i) respi |> Array.of_list |> Array.map snd
  in
  pred
  |> Array.mapi (fun i p ->
    let p_pred, p_resp = without i in
    let ms =
      { add_constant_column = false
      ; lambda_spec = Some (Spec lambda)
      }
    in
    let model = M.regress ~spec:ms p_pred ~resp:p_resp in
    resp.(i) -. M.eval model p)


(* Testing the accuracy of numerical algorithms is hard (and fun).
  Linear Regression provides an interesting example.
  There are two parameters that we use to tune the range of data to be
  generated by the tests. They give the user a gage on the accuracy of
  the algorithms.

  *)

let () =
  let add_random_test
    ?title ?nb_runs ?nb_tries ?classifier
    ?reducer ?reduce_depth ?reduce_smaller ?random_src gen f spec =
    Test.add_random_test_group "Regression"
      ?title ?nb_runs ?nb_tries ?classifier
      ?reducer ?reduce_depth ?reduce_smaller ?random_src gen f spec
  in

  (* Short cut evaluation. *)
  let lregress (a, b, pred)  =
    let resp = Array.map (fun x -> b *. x +. a) pred in
    U.regress ~resp pred
  in

  let simple_test_array =
    Gen.(lift (Array.init 100 float_of_int) "[0.;1.; ... ;99.]")
  in
  let random_test_array b = Gen.(array (make_int 2 50) (bfloat b)) in
  let simple_lrm_inputs b = Gen.(zip3 (bfloat b) (bnon_zero_float b) simple_test_array) in
  let lrm_inputs b      = Gen.(zip3 (bfloat b) (bnon_zero_float b) (random_test_array b)) in
  let lrm_gen b         = Gen.(map1 lregress U.describe (lrm_inputs b)) in

  (* The tests. *)

  (* Even though this test is awkward, if the [linear_model]' implementation is
     hidden, this guards to against an incorrect implementation. *)
  add_random_test
    ~title:"Lrm models evaluate like linear models"
    Gen.(zip2 (bnon_zero_float 1e8) (lrm_gen 1e8))
    (fun (x, lrm) ->
      let _ = U.describe lrm in
      U.eval lrm x)
    Spec.([ always =>
      (fun((x, lrm), y) ->
        equal_floats ~d:dx y (U.alpha lrm +. U.beta lrm *. x)) ]);

  add_random_test
    ~title:"Using linear regression on canonical [0.0 ... 99.0] data we can recover coefficients"
    ~nb_runs:1000
    (simple_lrm_inputs 1e8)
    lregress
    Spec.([ always =>
      (fun ((a, b, _), lrm) ->
      (*Printf.printf "1. our lrm %s a %f and b %f\n" (to_string lrm) a b; *)
        (equal_floats ~d:1e-5 a (U.alpha lrm)) && (equal_floats ~d:1e-5 b (U.beta lrm)))]);

  add_random_test
    ~title:"Using linear regression on random float data we can recover coefficients"
    ~nb_runs:1000
    (lrm_inputs 1e7)
    lregress
    Spec.([ always =>
      (fun ((a, b, _), lrm) ->
      (*Printf.printf "1. our lrm %s a %f and b %f\n" (to_string lrm) a b; *)
        (equal_floats ~d:1e-1 a (U.alpha lrm)) && (equal_floats ~d:1e-1 b (U.beta lrm)))]);

  (* TODO: clever test to react to variance in the predicted variable. *)

(*
  add_random_test
    ~title:"Confidence intervals are symmetric around evaluation point."
    Gen.(zip3 simple_lrm_gen grt_zer_lss_one bfloat)
    (fun (lrm, alpha_level, x) -> confidence_interval lrm ~alpha_level x)
    Spec.([ always =>
      (fun ((_lrm, _alpha, x), (lb, ub)) ->
        equal_floats ~d:1e-5 x (Util.midpoint lb ub))]);

  add_random_test
    ~title:"Prediction intervals grow as alpha increases."
      Gen.(zip3 simple_lrm_gen two_ordered bfloat)
    (fun (lrm, (a1, a2), x) ->
      (*Printf.printf "our lrm %s alpha %f %f and x %f\n" (to_string lrm) a1 a2 x; *)
      prediction_interval lrm ~alpha_level:a1 x,
      prediction_interval lrm ~alpha_level:a2 x)
    Spec.([always =>
      (fun ((_lrm, _alpha_pair, x), ((lb1, ub1), (lb2, ub2))) ->
        ub2 -. lb2 > ub1 -. lb1)])
*)

  let max_samples = 10 in
  let max_predictors = 3 in
  add_random_test
    ~title:"General can recover coefficients."
    Gen.(general_model 1e11 ~max_samples ~max_predictors)
    (fun (pred, coef, resp) ->
      let glm = M.regress ~resp pred in
      Vectors.equal ~d:1e-2 glm.coefficients coef)
    Spec.([just_postcond_pred is_true]);

  add_random_test
    ~title:"Residuals are just the difference between M.eval and resp"
    Gen.(general_model 1e5 ~max_samples ~max_predictors)
    (fun (pred, _, resp) ->
      let glm = M.regress ~resp pred in
      let r0 = glm.residuals.(0) in
      let e0 = resp.(0) -. M.eval glm pred.(0) in
      (*Printf.printf "%.20f\t%.20f\t%b\t%b\n" r0 e0 (r0 = e0) (equal_floats ~d:dx r0 e0)*) 
      equal_floats ~d:1e-6 r0 e0)
    Spec.([just_postcond_pred is_true]);

  add_random_test
    ~title:"Ridge coefficients are smaller."
    Gen.(general_model 1e11 ~max_samples ~max_predictors)
    (fun (pred, _, resp) ->
      let glm = M.regress ~resp pred in
      let m = Descriptive.mean resp in
      let lambda = Spec (m *. m) in    (* has to be big enough *)
      let ms = { add_constant_column = false; lambda_spec = Some lambda } in
      let rdg = M.regress ~spec:ms ~resp pred in
      let rd = Vectors.dot rdg.coefficients rdg.coefficients in
      let gd = Vectors.dot glm.coefficients glm.coefficients in
      rd < gd)
    Spec.([just_postcond_pred is_true]);

  add_random_test
    ~title:"Tikhonov... doesn't crash with something sensible."
    ~nb_runs:5 (* Limited because this example is time consuming *)
    Gen.(general_model 1e11 ~max_samples ~max_predictors)
    (fun (pred, _, resp) ->
      let r,c = Matrices.dim pred in
      let reg =
        Array.init r (fun i -> Array.init c (fun j ->
          if i = j then 1.0 else 0.0))
      in
      let opt = {regularizer = reg; lambda_spec = None} in
      let _t  = T.regress ~spec:opt ~resp pred in
      true)
    Spec.([just_postcond_pred is_true]);

  (* Computing LOOE has a subtraction of 1/S^2 - 1/lambda so it can be
     very imprecise when numbers get big. *)
  add_random_test
    ~title:"Svd, Leave-One-Out-Error is same as manually."
    Gen.(general_model 1e7 ~max_samples:5 ~max_predictors:3
         |> zip2 (bpos_float 1e7))
    (fun (lambda, (pred, _, resp)) ->
      let open Lacaml.D in
      let manually = looe_manually lambda pred resp in
      let svdally  = Vec.to_array (Svd.looe (Svd.svd (Mat.of_array pred)) (Vec.of_array resp) lambda) in
      Vectors.equal ~d:1e9 manually svdally)
    Spec.([just_postcond_pred is_true]);

  ()
